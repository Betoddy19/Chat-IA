import streamlit as st
import os
from dotenv import load_dotenv
from openai import OpenAI

# --- Configura√ß√£o ---
# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Inicializa o cliente da OpenAI UMA VEZ para evitar m√∫ltiplas inicializa√ß√µes
# Usamos st.cache_resource para garantir que o cliente seja criado apenas uma vez
# e reutilizado nas sess√µes do Streamlit, o que √© eficiente para objetos grandes.
@st.cache_resource
def get_openai_client():
    return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

client = get_openai_client()

# --- T√≠tulo da Aplica√ß√£o ---
st.title("ü§ñ Meu Chat-IA")

# --- Inicializa√ß√£o do Hist√≥rico da Conversa ---
# Usamos st.session_state para armazenar o hist√≥rico do chat
# Isso garante que o hist√≥rico persista entre as intera√ß√µes do usu√°rio
if "messages" not in st.session_state:
    st.session_state.messages = [] # Lista de dicion√°rios: [{"role": "user", "content": "Ol√°"}, {"role": "assistant", "content": "Oi!"}]

# --- Exibir Mensagens Anteriores ---
# Itera sobre o hist√≥rico e exibe as mensagens
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Campo de Entrada do Usu√°rio ---
# Este √© o campo onde o usu√°rio digita a pergunta
if prompt := st.chat_input("Pergunte algo √† IA..."):
    # Adiciona a pergunta do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Exibe a pergunta do usu√°rio no chat
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- L√≥gica de Resposta da IA ---
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # Placeholder para a resposta ser atualizada em tempo real
        full_response = ""
        
        try:
            # Chama a API da OpenAI com o hist√≥rico completo para contexto
            # Convertemos o hist√≥rico para o formato que a API espera
            stream = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
                stream=True, # Habilita o streaming para respostas mais r√°pidas
            )
            
            # Processa a resposta em streaming
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "‚ñå") # Adiciona cursor piscando
            
            message_placeholder.markdown(full_response) # Exibe a resposta final

            # Adiciona a resposta da IA ao hist√≥rico
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"‚ö†Ô∏è Erro ao se comunicar com a OpenAI: {e}")
            st.warning("Verifique sua chave de API, conex√£o com a internet e se h√° cr√©ditos na sua conta OpenAI.")